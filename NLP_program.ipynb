{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JACOBABRAHAMJANE/page/blob/main/NLP_program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NS4oz0jh5ZYS",
        "outputId": "72cbe6f1-12a7-49ef-8149-dcda7850b21f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# WORD TOKENIZATION\n",
        "import nltk # make sure to import nltk\n",
        "\n",
        "nltk.download('punkt_tab') # Download the 'punkt_tab' resource\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural Language Processing is fascinating!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "text = \"I am running this code in Jupyter!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PyjLSwmk5qCG",
        "outputId": "8b45e5b2-4b07-4138-e9d9-e90f93a12a3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n",
            "['I', 'am', 'running', 'this', 'code', 'in', 'Jupyter', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = ['running', 'runner', 'runs', 'easily', 'faster']\n",
        "stemmed_words= [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)\n",
        "words = ['eating', 'ate', 'goes', 'ease', 'read']\n",
        "stemmed_words= [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "918XDDmY5sHS",
        "outputId": "312a4c6a-88d6-4138-d5d0-f66a7c7283f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'run', 'easili', 'faster']\n",
            "['eat', 'ate', 'goe', 'eas', 'read']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# LEMMATIZATION\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['running', 'runner', 'flies', 'easily', 'better']\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos = 'v') for word in words]\n",
        "print( lemmatized_words)\n",
        "words = ['eating', 'ate', 'goes', 'ease', 'read']\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos = 'v') for word in words]\n",
        "print( lemmatized_words)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zFiun-Mw56Cp",
        "outputId": "ee374813-1a85-40da-e6ad-73eef42b6937"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'fly', 'easily', 'better']\n",
            "['eat', 'eat', 'go', 'ease', 'read']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STOP WORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english')) # Indented this line\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "sample_text = \"This is an example sentence demonstrating the removal of stop words.\"\n",
        "#sample_text = input()\n",
        "filtered_text = remove_stopwords(sample_text)\n",
        "print(\"Filtered Text:\", filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bReLo_Ee5_3v",
        "outputId": "ff6bfc28-4197-42ec-ff51-23b78b89bd5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Text: example sentence demonstrating removal stop words .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 spel checker\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PGEimkEm6VhL",
        "outputId": "2bf1867f-dcf3-4c71-d106-25db5fc66c88"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "def correct_spelling(text):\n",
        "    spell = SpellChecker()\n",
        "    words = text.split()\n",
        "    corrected_words = [spell.correction(word) if spell.correction(word)\n",
        "else word for word in words]\n",
        "    corrected_text = \" \".join(corrected_words)\n",
        "    return corrected_text\n",
        "text = \"Ths is a smple tst for speling corection.\"\n",
        "#text = input()\n",
        "corrected_text = correct_spelling(text)\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Corrected Text: \", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XZg9208w6eiN",
        "outputId": "c4cae063-fcf6-4b82-a3d9-ec50276ebf5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  Ths is a smple tst for speling corection.\n",
            "Corrected Text:  the is a simple test for spelling corrections\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 3WORD SEGMENTATION\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def segment_text(text):\n",
        "    words = word_tokenize(text) # Indented this line\n",
        "    return words # Indented this line\n",
        "text = \"Word segmentation is the process of dividing a string of written language into meaningful words.\"\n",
        "segmented_words = segment_text(text)\n",
        "print(segmented_words)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8972LFGF6q8E",
        "outputId": "b8d19dbe-8171-4039-f7b8-7fdbfcfc3b7a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Word', 'segmentation', 'is', 'the', 'process', 'of', 'dividing', 'a', 'string', 'of', 'written', 'language', 'into', 'meaningful', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SENTENCE SEGMENTATION\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \" Natural Language Processing is fascinating. It has many applications in AI.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "text = \" Natural Language Processing is used to develop AI Bots.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Szq7hSbk6uci",
        "outputId": "0e53ceee-fe47-40c8-dc4f-e65368b6c3fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Natural Language Processing is fascinating.', 'It has many applications in AI.']\n",
            "[' Natural Language Processing is used to develop AI Bots.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5SE4uIkv6x5w",
        "outputId": "e85ac86a-8ac1-455b-d1c4-9c2218f0dcdc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "\"Natural Language Processing is exciting\",\n",
        "\"Machine Learning is part of AI\",\n",
        "\"AI and NLP are closely related\"\n",
        "]\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "X_array = X.toarray()\n",
        "print(\"Feature Names (N-grams):\")\n",
        "print(feature_names)\n",
        "print(\"\\nCount Matrix:\")\n",
        "print(X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pbTe7RHj7Ch2",
        "outputId": "16d6def5-e820-4cc0-d6bf-6bc3919826a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (N-grams):\n",
            "['ai' 'ai and' 'and' 'and nlp' 'are' 'are closely' 'closely'\n",
            " 'closely related' 'exciting' 'is' 'is exciting' 'is part' 'language'\n",
            " 'language processing' 'learning' 'learning is' 'machine'\n",
            " 'machine learning' 'natural' 'natural language' 'nlp' 'nlp are' 'of'\n",
            " 'of ai' 'part' 'part of' 'processing' 'processing is' 'related']\n",
            "\n",
            "Count Matrix:\n",
            "[[0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0]\n",
            " [1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0]\n",
            " [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "!pip install nltk scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bSul_hLk7OjL",
        "outputId": "5d7a4668-ce74-410d-bbbd-67af6ca83ba7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5BAG OF WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "\"Natural Language Processing is exciting\",\n",
        "\"Machine Learning is part of AI\",\n",
        "\"AI and NLP are closely related\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "X_array = X.toarray()\n",
        "print(\"Feature Names (BoW):\")\n",
        "print(feature_names)\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N0gOCsrR7I2T",
        "outputId": "650e00c4-679a-4adb-b519-58eabb030e73"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (BoW):\n",
            "['ai' 'and' 'are' 'closely' 'exciting' 'is' 'language' 'learning'\n",
            " 'machine' 'natural' 'nlp' 'of' 'part' 'processing' 'related']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 0 1 1 1 0 0 1 0 0 0 1 0]\n",
            " [1 0 0 0 0 1 0 1 1 0 0 1 1 0 0]\n",
            " [1 1 1 1 0 0 0 0 0 0 1 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS TAGGING\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the correct resource\n",
        "sentence = \"Machine learning enables computers to learn from data.\"\n",
        "words = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tagging Result:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bt5BRKTk7bTE",
        "outputId": "65972a18-ad2d-4a74-f102-443be1ee8742"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tagging Result:\n",
            "[('Machine', 'NN'), ('learning', 'VBG'), ('enables', 'NNS'), ('computers', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('from', 'IN'), ('data', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "import nltk\n",
        "from nltk.tokenize import MWETokenizer, word_tokenize\n",
        "mwe_tokenizer = MWETokenizer([(\"New\", \"York\"), (\"machine\", \"learning\"), (\"artificial\", \"intelligence\")])\n",
        "text = \"New York is a hub for machine learning and artificial intelligence research.\"\n",
        "tokens = word_tokenize(text)\n",
        "mwe_tokens = mwe_tokenizer.tokenize(tokens)\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Multiword Tokens:\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GRvvBHTI7tkP",
        "outputId": "327e4d1a-e2e7-4be9-e6a7-b5a1c7cb4185"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['New', 'York', 'is', 'a', 'hub', 'for', 'machine', 'learning', 'and', 'artificial', 'intelligence', 'research', '.']\n",
            "Multiword Tokens: ['New_York', 'is', 'a', 'hub', 'for', 'machine_learning', 'and', 'artificial_intelligence', 'research', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Indented this line\n",
        "    text = re.sub(r'\\W+', ' ', text)   # Indented this line\n",
        "    words = word_tokenize(text)   # Indented this line\n",
        "    words = [word for word in words if word not in stopwords.words('english')]   # Indented this line\n",
        "    return \" \".join(words) # Indented this line\n",
        "sample_text = \"Hello! This is an example of Natural Language Processing (NLP) cleaning.\"\n",
        "cleaned_text = clean_text(sample_text)\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Xsd4_meO73yN",
        "outputId": "91cf8f83-caff-4770-efd7-3e90e51e3f1e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Hello! This is an example of Natural Language Processing (NLP) cleaning.\n",
            "\n",
            "Cleaned Text:\n",
            "hello example natural language processing nlp cleaning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "\"Natural Language Processing is exciting\",\n",
        "\"Machine Learning is a part of AI\",\n",
        "\"AI and NLP are closely related\"\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "X_array = X.toarray()\n",
        "print(\"Feature Names (TF-IDF):\")\n",
        "print(feature_names)\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(X_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "os0myfGi8DXT",
        "outputId": "89754f16-d0ef-4f2b-87fc-5337ed7ec3b7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (TF-IDF):\n",
            "['ai' 'and' 'are' 'closely' 'exciting' 'is' 'language' 'learning'\n",
            " 'machine' 'natural' 'nlp' 'of' 'part' 'processing' 'related']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.46735098 0.35543247\n",
            "  0.46735098 0.         0.         0.46735098 0.         0.\n",
            "  0.         0.46735098 0.        ]\n",
            " [0.3349067  0.         0.         0.         0.         0.3349067\n",
            "  0.         0.44036207 0.44036207 0.         0.         0.44036207\n",
            "  0.44036207 0.         0.        ]\n",
            " [0.32200242 0.42339448 0.42339448 0.42339448 0.         0.\n",
            "  0.         0.         0.         0.         0.42339448 0.\n",
            "  0.         0.         0.42339448]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "def get_wordnet_info(word):\n",
        "    synonyms = []\n",
        "    antonyms = []\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if not synsets:\n",
        "        return f\"No WordNet data found for '{word}'\"\n",
        "    for syn in synsets:\n",
        "        synonyms.extend(syn.lemma_names())\n",
        "        print(f\"Definition: {syn.definition()}\")\n",
        "        print(f\"Example: {syn.examples()}\\n\")\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                antonyms.append(lemma.antonyms()[0].name())\n",
        "    return {\n",
        "        \"Synonyms\": set(synonyms),\n",
        "        \"Antonyms\": set(antonyms)\n",
        "    }\n",
        "word = \"happy\"\n",
        "word_info = get_wordnet_info(word)\n",
        "print(\"\\nSynonyms:\", word_info[\"Synonyms\"])\n",
        "print(\"Antonyms:\", word_info[\"Antonyms\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UoV-dATR8Oex",
        "outputId": "6d6f9bdd-4a35-414a-99c6-9692c5bd4186"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: enjoying or showing or marked by joy or pleasure\n",
            "Example: ['a happy smile', 'spent many happy days on the beach', 'a happy marriage']\n",
            "\n",
            "Definition: marked by good fortune\n",
            "Example: ['a felicitous life', 'a happy outcome']\n",
            "\n",
            "Definition: eagerly disposed to act or to be of service\n",
            "Example: ['glad to help']\n",
            "\n",
            "Definition: well expressed and to the point\n",
            "Example: ['a happy turn of phrase', 'a few well-chosen words']\n",
            "\n",
            "\n",
            "Synonyms: {'felicitous', 'well-chosen', 'happy', 'glad'}\n",
            "Antonyms: {'unhappy'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "!pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tY4b--GR8dZ3",
        "outputId": "1ca711fb-4782-4998-c057-00b983a7a811"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "states = [\"Noun\", \"Verb\", \"Adjective\"]\n",
        "num_states = len(states)\n",
        "observations = [\"dog\", \"barks\", \"happy\", \"cat\", \"runs\", \"blue\"]\n",
        "num_observations = len(observations)\n",
        "state_dict = {state: i for i, state in enumerate(states)}\n",
        "obs_dict = {obs: i for i, obs in enumerate(observations)}\n",
        "model = hmm.MultinomialHMM(n_components=num_states, n_iter=1000)\n",
        "model.startprob_ = np.array([0.5, 0.3, 0.2])\n",
        "model.transmat_ = np.array([\n",
        "    [0.6, 0.3, 0.1],\n",
        "    [0.2, 0.7, 0.1],\n",
        "    [0.3, 0.3, 0.4]\n",
        "])\n",
        "model.emissionprob_ = np.array([\n",
        "    [0.6, 0.2, 0.2],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.3, 0.5]\n",
        "])\n",
        "train_sequences = [\n",
        "    [\"dog\", \"barks\", \"happy\"],\n",
        "    [\"cat\", \"runs\", \"blue\"],\n",
        "    [\"dog\", \"runs\", \"happy\"],\n",
        "]\n",
        "train_data = np.array([[obs_dict[word]] for seq in train_sequences for\n",
        "word in seq])\n",
        "model.fit(train_data)\n",
        "test_seq = [\"dog\", \"barks\", \"blue\"]\n",
        "test_data = np.array([[obs_dict[word]] for word in test_seq])\n",
        "hidden_states = model.predict(test_data)\n",
        "predicted_tags = [states[state] for state in hidden_states]\n",
        "print(\"\\n Test Observations:\", test_seq)\n",
        "print(\" Predicted POS Tags:\", predicted_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R-PTfhuF8UCL",
        "outputId": "9ccd8a23-1d44-47c5-9096-ae5d014e5746"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
            "WARNING:hmmlearn.base:Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Observations: ['dog', 'barks', 'blue']\n",
            " Predicted POS Tags: ['Noun', 'Verb', 'Adjective']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "states = [\"Noun\", \"Verb\", \"Adjective\"]\n",
        "observations = [\"dog\", \"barks\", \"happy\"]\n",
        "obs_map = {word: i for i, word in enumerate(observations)}\n",
        "start_prob = np.array([0.5, 0.3, 0.2])\n",
        "trans_prob = np.array([\n",
        "    [0.6, 0.3, 0.1],\n",
        "    [0.2, 0.7, 0.1],\n",
        "    [0.3, 0.3, 0.4]\n",
        "])\n",
        "emission_prob = np.array([\n",
        "    [0.7, 0.2, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "    [0.2, 0.3, 0.5]\n",
        "])\n",
        "obs_seq = [\"dog\", \"barks\", \"happy\"]\n",
        "\n",
        "def viterbi(obs_seq, states, start_prob, trans_prob, emission_prob):\n",
        "    num_obs = len(obs_seq)\n",
        "    num_states = len(states)\n",
        "    viterbi_matrix = np.zeros((num_states, num_obs))\n",
        "    backpointer = np.zeros((num_states, num_obs), dtype=int)\n",
        "    for s in range(num_states):\n",
        "        viterbi_matrix[s, 0] = start_prob[s] * emission_prob[s, obs_map[obs_seq[0]]]\n",
        "        backpointer[s, 0] = 0\n",
        "    for t in range(1, num_obs):\n",
        "        for s in range(num_states):\n",
        "            max_prob, prev_state = max(\n",
        "                (viterbi_matrix[prev_s, t-1] * trans_prob[prev_s, s] * emission_prob[s, obs_map[obs_seq[t]]], prev_s)\n",
        "                for prev_s in range(num_states)\n",
        "            )\n",
        "            viterbi_matrix[s, t] = max_prob\n",
        "            backpointer[s, t] = prev_state\n",
        "    # Corrected indentation here\n",
        "    best_last_state = np.argmax(viterbi_matrix[:, -1])\n",
        "    best_path = [best_last_state]\n",
        "    for t in range(num_obs - 1, 0, -1):\n",
        "        best_path.insert(0, backpointer[best_path[0], t])\n",
        "        best_state_sequence = [states[i] for i in best_path]\n",
        "    return best_state_sequence\n",
        "\n",
        "predicted_tags = viterbi(obs_seq, states, start_prob, trans_prob, emission_prob)\n",
        "print(\"Observations:\", obs_seq)\n",
        "print(\"Predicted POS Tags:\", predicted_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sN3SSZF78r9m",
        "outputId": "249274a7-5cdd-4729-f6b0-4136b49169a7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observations: ['dog', 'barks', 'happy']\n",
            "Predicted POS Tags: ['Noun', 'Verb', 'Verb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "38kDtlLR9mM9",
        "outputId": "6adcd441-d62c-4f10-e08e-b7f3acff58d8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=05a01f4f0cfec6ab176b1ec6d0bc9bcbc4c3bf2de2848796a6623ebbedaa695b\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f2/e0/d578821d723b473d18610ea93810e4a5402463919f07e603d9\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.13 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "def translate_text(text, src_lang=\"auto\", dest_lang=\"fr\"):\n",
        "    try: # Indented this line to be part of the function definition\n",
        "        translated = translator.translate(text, src=src_lang, dest=dest_lang)\n",
        "        return translated.text\n",
        "    except Exception as e: # Indented this line to be part of the function definition\n",
        "        return f\"Translation Error: {str(e)}\"\n",
        "input_text = \"Hello, how are you?\"\n",
        "translated_text = translate_text(input_text, src_lang=\"en\", dest_lang=\"hi\")\n",
        "translated_text1 = translate_text(input_text, src_lang=\"en\", dest_lang=\"te\")\n",
        "print(f\"Original: {input_text}\")\n",
        "print(f\"Translated: {translated_text}\")\n",
        "print(f\"Translated: {translated_text1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mpOy2ouh9Jeb",
        "outputId": "cd7a540f-7acf-480f-c18f-e5e5d5768e5a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Hello, how are you?\n",
            "Translated: नमस्ते, आप कैसे हैं?\n",
            "Translated: హలో, మీరు ఎలా ఉన్నారు?\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx9li9kR6hlg5KaqWTYsmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}